{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные модели\n",
    "from sklearn import tree #деревья решений\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import preprocessing #предобработка\n",
    "from sklearn.model_selection import train_test_split #сплитование выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем нужный модуль k-means-кластеризации\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "# импортируем метрику полноты\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "# передаем предсказанную информацию к какому кластеру относятся объекты датасета и правильные ответы, \n",
    "# подсчитываем метрику\n",
    "print(round(v_measure_score(labels_true=[1, 2, 2, 1, 0], labels_pred=[1, 0, 2, 1, 2]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из модуля decomposition библиотеки sklearn импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# создаём объект класса PCA\n",
    "# n_components — задаём количество компонентов для проведения трансформации\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# обучаем модель на данных X\n",
    "pca.fit(X)\n",
    "# применяем уменьшение размерности к матрице X\n",
    "pca.transform(X)\n",
    "\n",
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u1\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# загрузим датасет MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = fetch_openml(\"mnist_784\")\n",
    "# загрузим признаки в переменную X  \n",
    "X = dataset['data']\n",
    "# загрузим «ответы» в переменную y\n",
    "y = dataset['target']\n",
    "# разделим данные с помощью sklearn на данные для обучения и теста\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# импортируем StandardScaler для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# создадим объект класса StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "# трансформируем датасеты train_x и test_x\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# создадим объект класса PCA\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(train_x)\n",
    "# уменьшим размерность данных\n",
    "train_x_pca = pca.transform(train_x)\n",
    "test_x_pca = pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[0]))\n",
    "print(len(train_x_pca[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения 154.30163431167603, метрика модели 0.926\n",
      "Модель, построенная на всех исходных признаках. Время обучения 405.7208454608917, метрика модели 0.9187142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "# напишем функцию, которая на вход принимает X и y, а возвращает модель и время\n",
    "def get_time_and_accuracy(train_x, train_y, test_x, test_y):\n",
    "    # создадим объект класса LogisticRegression\n",
    "    log_reg_model = LogisticRegression(max_iter=1000)\n",
    "    # запишем время с начала эпохи в секундах до обучения модели\n",
    "    start_time = time()\n",
    "    # обучим модель\n",
    "    log_reg_model.fit(train_x, train_y)    \n",
    "    # запишем время с начала эпохи в секундах после обучения\n",
    "    end_time = time()\n",
    "    # подсчитаем время, потраченное на обучение модели\n",
    "    delta_time = end_time-start_time\n",
    "    # предскажем на тестовых данных\n",
    "    y_pred = log_reg_model.predict(test_x)\n",
    "    # посчитаем скор для тестового предсказания\n",
    "    score = accuracy_score(test_y, y_pred)\n",
    "    # вернём время, потраченное на обучение, и качество полученной модели\n",
    "    return delta_time, score\n",
    "\n",
    "model_pca_time, model_pca_acc = get_time_and_accuracy(train_x_pca, train_y, test_x_pca, test_y)\n",
    "print(f\"Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения {model_pca_time}, метрика модели {model_pca_acc}\")\n",
    "# Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения 54.12072825431824, метрика модели 0.9255714285714286\n",
    "\n",
    "model_time, model_acc = get_time_and_accuracy(train_x, train_y, test_x, test_y)\n",
    "print(f\"Модель, построенная на всех исходных признаках. Время обучения {model_time}, метрика модели {model_acc}\")\n",
    "# Модель, построенная на всех исходных признаках. Время обучения 108.04033303260803, метрика модели 0.9187142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5669727e+01,  2.8668145e-02],\n",
       "       [ 3.1366806e+01, -1.8563803e+01],\n",
       "       [ 1.7223909e+01,  3.7413433e+01],\n",
       "       ...,\n",
       "       [ 4.2127867e+00,  2.0259901e+01],\n",
       "       [ 1.0290709e+01,  1.1728876e+01],\n",
       "       [ 2.9229694e+01,  1.6523983e+01]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импортируем класс TSNE из модуля manifold библиотеки sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# создаём объект класса TSNE\n",
    "# n_components — размерность нового пространства\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "# обучаем модель на данных X и производим трансформацию\n",
    "tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важные параметры для запуска:\n",
    "\n",
    "n_components — размерность нового пространства.  \n",
    "perplexity — один из важнейших параметров для запуска. Этот параметр описывает ожидаемую плотность вокруг точки. Таким образом мы можем устанавливать соотношение ближайших соседей к точке. Если датасет большой, стоит установить большее значение perplexity. Обычно используют значения в диапазоне от 5 до 50.  \n",
    "n_iter — количество итераций для оптимизации.  \n",
    "random_state — так как в алгоритме есть случайность, задание random_state позволяет от запуска к запуску получать одинаковые результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
